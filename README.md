# dns-analysis
| Notebook/script | Used for | Required input(s) | Output(s)/result(s)|
| --------------- | -------- | ----------------- | ------------------ |
| youtube-all-proto-single-rslv-and-vm.ipynb | Generate CDFs and boxplots (startup delay, delay to optimal quality, buffering events, stall times) for measurement run in week 8 of one video over 5 DNS protocols againt AdGuard DNS for 720p-4k. | web-performance-youtube-single-vm-5-proto.db (measurement results generated by run_youtube_measurements.sh) | Plots as PDFs in figures directory. |
| youtube-get-itags.ipynb   | deprecated notebook that does not work for chrome/chromium, only safari to extract itag metadata | none | itag metadata for given video IDs. |
| youtube-video-quality-one-vm-test.ipynb | Figure out how setting parameters in the youtube iframe API affects the optimal resolution for a measurement and whether buffering is somewhat deterministic across measurement runs with the same setup. | Measurement run of one protocol (UDP) against a single resolver iterating over all possible suggested qualities and setting possible resolutions as the iframe player's width and height. | Look inside the notebook. |
| web-performance.ipynb | Generate web performance plots for each vantage point. | measurements against five per-vantage-point-reachable DNS resolvers (from the large list) for top websites (measurements table) | <ul><li>boxplots: PLT/transferSize, PLT/decodedBodySize, PLT, PLT vs max(all DNS lookup times of msm), max(all DNS lookup times), RT ((for TCP and UDP the average DNS lookup time/for QUIC and TLS and HTTPS the max DNS lookup time) divided by the average UDP DNS lookup time for a given upstream and vantage point (taken from a UDP measurement within 30 min of the measurement)), FCP, max(lookups) vs (PLT - max(lookups)), max(lookups) vs (FCP - max(lookups))</li><li>heatmaps: PLT/decodedBodySize, PLT/transferSize, PLT, correlations using pandas.DataFrame.corr()</li></ul>|
| Vantage Points Map.ipynb | Create map of the vantage points | none | Folium-generated HTML |
| reverse.py | reverse DNS lookup function that is not used anywhere? | none | none |
| QUIC_Comparison.ipynb | extract common name or first subject alternative name for each item in certs.csv, plot QUIC handshake times for resolver IP addresses and common names | certs.csv (presumably certificates of DNS resolvers), measurements-all.db, requires tldextract | QUIC, TLS, HTTPS, TCP handshake plots by upstream resolver |
| QLOG.ipynb | some qlot data (pre?) processing | *.qlog files | qlog-stats.csv |
| preprocess-adoption.py | counts how many resolvers do DoQ over which port | results/*.csv | prints on CLI |
| Performance.ipynb | DNS performance over 5 Protocols for each vantage point. Servers with multiple DoQ ports collapsed into one. | dns_measurements table | total time, resolve time, handshake time, setup time, handshake in terms of round trips (traceroute)|
| Performance-Top5.ipynb | DNS performance of the top five resolvers over five DNS protocols for each vantage point. | dns_measurements_clean table (generated by Performance.ipynb) | total time, resolve time, handshake time, setup time, handshake in terms of round trips (traceroute)|
| Meta.ipynb | List all resolvers by average DoQ total time. Generates the top five (shortest mean total time) resolvers per vantage point, at most one resolver per AS, which is used in Performance-Top5.ipynb. Analyses errors, protocol versions and server certificates briefly. | dns_measurements table | Generates a map for each vantage point of the approximate locations of its top five resolvers. |
| Meta_Grouped.ipynb | Error, RCODE analysis and certificate sizes | dns_measurements table | ? |
| fluctuations.py | Weekly fluctations of DoQ-capable resolvers | results/*.csv presumably from zmap and/or verify-doq | prints to CLI |
| Detailed_Handshake.ipynb | QUIC (vs TLS) handshake analysis (handshake duration in terms of traceroute rtt). Bucketed into different RTT bands. | dns_measurements table and qlog-results.json | handshake duration by RTT plots
| compare-misc.ipynb | Something with ASes and the different vantage points | ? | ? |

overall setup:
1. run zmap fork on 784, 853 and 8853
2. run verify-doq to make sure QUIC works server-side
3. run dns-measurements main.go (workflows/protocols.go manages token store, source port and QUIC version), which runs dnsperf's client, which uses a modified quic-go module that can set its source port
4. run web-performance/test_servers.sh to get the first five reachable resolvers out of the (verified?) full list of resolvers
5. run web-performance/run_measurements.sh to measure web performance
6. run Meta.ipynb -> Performance.ipynb -> Performance-Top5.ipynb for DNS performance
7. run web-performance.ipynb for Web performance
